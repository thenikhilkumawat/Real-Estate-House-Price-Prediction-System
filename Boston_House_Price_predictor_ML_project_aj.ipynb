{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK9JsC7gBUQc"
   },
   "source": [
    "# Boston House Price Predictor using ML\n",
    "\n",
    "Project Information:\n",
    "This is my 1st Machine learning Project. I am taking data of boston house pricing from kaggle and apllying machine learning\n",
    "to predict prices if we have provided with new feature of a house.\n",
    "\n",
    "Date - 10/07/2022\n",
    "\n",
    "Time - 2:56 AM\n",
    "\n",
    "Author - Abhijit Kumar(Nit Durgapur CSE)\n",
    "\n",
    "##Atrributes of the data\n",
    "\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to five Boston employment centres\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per $10,000\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "56u7st_W1t0_"
   },
   "outputs": [],
   "source": [
    "#import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split #used for spliting dataset into training and testing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "L-0cTllc4eay",
    "outputId": "0191816e-4456-4669-acc9-bc99f79f3153"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRIM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINDUS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHAS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTRATIO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#reading data in csv file and creating a dataframe\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m csv_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m csv_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data.csv'"
     ]
    }
   ],
   "source": [
    "feature_names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
    "#reading data in csv file and creating a dataframe\n",
    "csv_df=pd.read_csv(\"/data.csv\",names=feature_names)\n",
    "csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rZZmOvBFYJV"
   },
   "source": [
    "##Analyzing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M8kMK6JQFqdL"
   },
   "outputs": [],
   "source": [
    "# Steps\n",
    "# 1) csv_df.info()\n",
    "# 2) csv_df['column_name'].value_counts()\n",
    "# 3) csv_df.describe()\n",
    "# 4) csv_df.hist(bins=50,figsize=(20,10))\n",
    "## Here we also seperate our data into features and labels to simplify the process of splitting\n",
    "labels_data=csv_df['MEDV'].copy(deep=True)\n",
    "features_data=csv_df.drop('MEDV',axis=1).copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6vZpPI4KfOE"
   },
   "source": [
    "##Spliting Training and testing data\n",
    "At this point we should  split our data into training and testing so that we will not conclude unnecessary patterns from all data.\n",
    "\n",
    "*Proablem to be taken care of:-*\n",
    "\n",
    "But we have to take take of uniformness of data while spliting Eg:- if a feature has binary values and 100 times it exists as 1 and 20 times it exists as 0 .So there might be a case that we accidently split the data in such a way that all ones came in training so our model will make wrong assumtion (our model think that there is only one value possible for this feature.)\n",
    "\n",
    "Solution ‚û°\n",
    "\n",
    "We use stratified sampling.for that we use inbuilt class of sklearn that is train_test_split(We can also use StratifiedShuffleSplit\n",
    "`(sklearn.model_selection.train_test_split)`\n",
    "\n",
    "##inside this train_test_split method there is option for stratified spliting.\n",
    "`sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mi6e3_gfJp9T"
   },
   "outputs": [],
   "source": [
    "#here we done stratified sampling on the basis of 'CHAS' feature\n",
    "features,features_test,labels,labels_test=train_test_split(features_data,labels_data,test_size=0.2,train_size=0.8,random_state=42,shuffle=True,stratify=features_data['CHAS'])\n",
    "X_test=features_test.to_numpy()\n",
    "Y_test=labels_test.to_numpy()\n",
    "#From here on we do not touch our test datas we only work on our trianing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9cREJ2ZO8K9"
   },
   "source": [
    "### Exploring Correlation\n",
    "Now we try to find or explore the correlation of every feature with label for that we use df.corrwith() function .it output float value between -1 to 1 where -1 -> very strong negetive relation and +1 -> very strong positve relation.\n",
    "We can also use seaborn heatmap plotting or scatter_matrix plotting using pandas.plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "R42iHWw1M5eA",
    "outputId": "0d9f426f-6642-42b9-ef01-9be11770e368"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nfrom pandas.plotting import scatter_matrix\\n#its better that you use attributes and see plots and then change items in it and again see plots its better otherwise we can see heatmap\\nattributes=['CRIM','RM','AGE','MEDV']\\nscatter_matrix(fulldata_train[attributes],alpha=0.8,figsize=(12,10))\\nplt.show()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also created a fulldata_train by adding features and labels for future operations\n",
    "fulldata_train=features.join(labels).copy(deep=True)\n",
    "#finding and plottinf correlation matrix\n",
    "corr_matrix=fulldata_train.corr()\n",
    "#heatmap plotting using seaborn\n",
    "'''\n",
    "import seaborn as sns\n",
    "sns.heatmap(corr_matrix.loc[:,'MEDV':])\n",
    "'''\n",
    "#scatter_matrix plotting using pandas.plotting\n",
    "'''\n",
    "from pandas.plotting import scatter_matrix\n",
    "#its better that you use attributes and see plots and then change items in it and again see plots its better otherwise we can see heatmap\n",
    "attributes=['CRIM','RM','AGE','MEDV']\n",
    "scatter_matrix(fulldata_train[attributes],alpha=0.8,figsize=(12,10))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP2DJWysZVG2"
   },
   "source": [
    "##Taking care of missing attributes\n",
    "you have three option to deal with it\\\n",
    "1) Get rid of the missing data points\\\n",
    "`df.dropna(subset=[\"attribute_name\"])` #it does not change original df so for changing in original df we have to use inplace=True\\\n",
    "2)Get rid of whole attribute\\\n",
    "`df.drop(\"attribute_name\",axis=1,inplace=True)`\\\n",
    "3)reaplace NA value with some other value (that do not chaange in your model results)\\\n",
    "(we are replacing here with median)\\\n",
    "`median=df[\"atrribute_name\"].median()\n",
    "df[\"attribute_name\"].fillna(median)`\n",
    "\n",
    "##But for doing this task we already have an inbuilt method (Imputer)\n",
    "`from sklearn.impute import SimpleImputer`\\\n",
    "`imputer=SimpleImputer(strategy=\"median\")`\\\n",
    "`imputer.fit(df)`\\\n",
    "`imputed_array=imputer.transform(df)` #this is an ndarray so we can convert it into pandas dataframe\\\n",
    "`tranformed_df=pd.DataFrame(imputed_array,columns=df.columns)` #tranformed_df stores df with all NA replaced by their respective medians`\\\n",
    "we can also do above steps by making a pipeline (recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "2axqqSm-t6m5",
    "outputId": "35294f9d-a529-4bcd-aace-9133978a1aee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# we can do this in pipeline\\n\\nfrom sklearn.impute import SimpleImputer\\nimputer=SimpleImputer(strategy=\"median\")\\nimputer.fit(fulldata_train)\\nimputed_array=imputer.transform(fulldata_train)\\ntranformed_df=pd.DataFrame(imputed_array,columns=fulldata_train.columns)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# we can do this in pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(fulldata_train)\n",
    "imputed_array=imputer.transform(fulldata_train)\n",
    "tranformed_df=pd.DataFrame(imputed_array,columns=fulldata_train.columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrK3CeOBcy5l"
   },
   "source": [
    "##Scikit-learn Design\n",
    "Primary three type of objects:-\\\n",
    "**1) Estimators**\\\n",
    "It estimates some parameters based on the datasets ex:- Imputer\\\n",
    "It has a fit method-> fits the dataset and calculate internal parameters and tranform method.\\\n",
    "**2) Tranformers**\\\n",
    "transform method takes input and return output based on the learning from fit() of estimiator.\\\n",
    "It also as a convineance function called `fit_transform()` {faster} which fits and then transform simultaneously.\\\n",
    "**3) Predictors**\\\n",
    "LinearRegression model is an Exaple of predictor.It has two common functions a)`fit()` -> fits the training data and create model b)`predict()`-> predicts the output based on the learning of the fit.\\\n",
    "it also gives the `score()` function which will evaluate the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NZeyeT7epFS"
   },
   "source": [
    "## Features-scaling\n",
    "**Why we need this concept?**\\\n",
    "Value of different features can be very diffrent ex:-faetureA values can range in [0,1] and value of featureB can range in [-1000,1000] so for better modelling we somehow try that if we can scale all feature in same range that's why we use feature scaling.\\\n",
    "**Primary there are two types of feature scaling.**\\\n",
    "1) Min-Max Scaling (Normalisation)\\\n",
    "Ex:- let there be a featureA which minmum value is minm and maximum value is maxm.so for each value val of featureA  we do\\\n",
    "scaled_val=(val-min)/(max-min) (#range of scaled_val is [0,1])\\\n",
    "sklearn provides a class called `MinMaxScaler` for minmax scaling.\\\n",
    "2) Standardization\\\n",
    "val->values of feature, mean-> mean of all values of feature , std-> standard deviation of feature.\\\n",
    "scaled_value=(val-mean)/(std)\\\n",
    "sklearn provides a class called `StandardScaler` for this scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "o0a7njZBelzP",
    "outputId": "57adfced-7053-4438-9cd7-44ded2fd4549"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# we can do this in pipeline\\n\\nfrom sklearn.preprocessing import StandardScaler\\nstandard_scaler=StandardScaler()\\nfeatures=pd.DataFrame(standard_scaler.fit_transform(features),column=features.columns)\\nfeatures.head()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# we can do this in pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler=StandardScaler()\n",
    "features=pd.DataFrame(standard_scaler.fit_transform(features),column=features.columns)\n",
    "features.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNCSPXB9v4h0"
   },
   "source": [
    "#Creating a Pipeline\n",
    "In a pipeline we add diffrent processes.a pipeline takes the data and run the process on it and pass the output to the next process to execute.\\\n",
    "`from sklearn.pipeline import Pipeline`\\\n",
    "`from sklearn.preprocessing import StandardScaler`\\\n",
    "`from sklearn.model_selection import SimpleImputer`\\\n",
    "`my_pipeline=Pipeline([`\\\n",
    "  `('imputer',SimpleImputer(strategy=\"median\")),#We can add as many process as we want`\\\n",
    "  `('std_scaler',StandardScaler())`\\\n",
    "`])`\\\n",
    "After that we can directly use fit_tranform method of pipeline.and this fit transform return a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BNwQo_kryS4P"
   },
   "outputs": [],
   "source": [
    "# We will crete a pipeline here and add Imputer and scaling into pipeline\n",
    "my_pipeline=Pipeline([\n",
    "('imputer',SimpleImputer(strategy=\"median\")),#We can add as many process as we want\n",
    "('std_scaler',StandardScaler())\n",
    "])\n",
    "'''\n",
    "# Parllel plotting of dataframe\n",
    "fulldata_train=pd.DataFrame(my_pipeline.fit_transform(fulldata_train),columns=fulldata_train.columns)\n",
    "ax=pd.plotting.parallel_coordinates(fulldata_train,'MEDV')\n",
    "legend =ax.legend()\n",
    "legend.remove()\n",
    "plt.show()\n",
    "'''\n",
    "# fit and transform the pipeline on features\n",
    "#we are just giving a new name to the features as X_train and labels to Y_train\n",
    "X_train=my_pipeline.fit_transform(features)\n",
    "Y_train=labels.to_numpy()\n",
    "\n",
    "# this below step is very important because if our training feature have passed through pipeline then our testing features should also\n",
    "# pass through pipeline\n",
    "X_test=my_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XlMqior3wY4"
   },
   "source": [
    "#Select your desired model for modelling\n",
    "Example models for Regression\\\n",
    "**DecisionTreeRegressor**\\\n",
    "syntax\\\n",
    "`from sklearn.tree import DecisionTreeRegressor`\n",
    "\n",
    "**Gradient Boosting Regression**\\\n",
    "Syntax\\\n",
    "`from sklearn.ensemble import GradientBoostingRegressor`\\\n",
    "\n",
    "**Elastic Net Regression**\\\n",
    "Syntax\\\n",
    "`from sklearn.linear_model import ElasticNe`t\n",
    "\n",
    "**Stochastic Gradient Descent Regression**\\\n",
    "Syntax\\\n",
    "`from sklearn.linear_model import SGDRegressor`\n",
    "\n",
    "**Support Vector Machine**\\\n",
    "Syntax\\\n",
    "`from sklearn.svm import SVR`\n",
    "\n",
    "**Bayesian Ridge Regression**\\\n",
    "Syntax\\\n",
    "`from sklearn.linear_model import BayesianRidge`\n",
    "\n",
    "**CatBoost Regressor**\\\n",
    "Syntax\\\n",
    "`from catboost import CatBoostRegressor`\n",
    "\n",
    "**Kernel Ridge Regression**\\\n",
    "Syntax\\\n",
    "`from sklearn.kernel_ridge import KernelRidge`\n",
    "\n",
    "**Linear Regression**\\\n",
    "Syntax\\\n",
    "`from sklearn.linear_model import LinearRegression`\n",
    "\n",
    "**KNeighbors Regressor**\\\n",
    "Syntax\\\n",
    "`from sklearn.neighbors import KNeighborsRegressor`\n",
    "\n",
    "**RandomForestRegressor**\\\n",
    "Syntax\\\n",
    "`from sklearn.ensemble import RandomForestRegressor`\n",
    "\n",
    "**XGBoost Regressor**\\\n",
    "Syntax\\\n",
    "`from xgboost.sklearn import XGBRegressor`\n",
    "\n",
    "**LGBM Regressor**\\\n",
    "Syntax\\\n",
    "`from lightgbm import LGBMRegressor`\\\n",
    "**You can allways check for k-fold cross validation (to check if our model get overfitted on training dataset)**\\\n",
    "K-fold cross validation makes  k groups randomly in our dataset and run our model on it and calculate the error for every fold and return it.\\\n",
    "Syntax\\\n",
    "`from sklearn.model_selection import cross_val_score`\\\n",
    "`cross_val_score(model/estimator,x,y,scoring=\"neg_mean_squared_error\",cv=10(->groups of folding))`\\\n",
    "it return neg of mean sqaured error so we have to make it positive and then take its root to get root mean squred  error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZeV19sr1YiR",
    "outputId": "c8e01cad-2781-489a-c34c-07bb0ddbdd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rmse =  3.3789639901985953\n",
      "Rmse cross val scores = \n",
      " [2.87667815 2.90645052 4.44626112 2.62887072 3.43234611 2.54121222\n",
      " 4.512502   3.27714691 3.44440933 3.21893902]\n"
     ]
    }
   ],
   "source": [
    "# import your model and start modelling\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model=RandomForestRegressor()\n",
    "model.fit(X_train,Y_train)\n",
    "# model.score(features,labels)\n",
    "# model.coef_\n",
    "# model.intercept_\n",
    "Y_predict=model.predict(X_test)\n",
    "\n",
    "# calulating root mean squared error\n",
    "rmse=np.sqrt(metrics.mean_squared_error(Y_test,Y_predict))\n",
    "print(\"Rmse = \",rmse)\n",
    "\n",
    "#Calculating K-fold cross validation (to check if our model got overfitted)\n",
    "rmse_cross_val_scores=np.sqrt(-1*cross_val_score(model,X_train,Y_train,scoring=\"neg_mean_squared_error\",cv=10))\n",
    "print(\"Rmse cross val scores = \\n\",rmse_cross_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_GNnzcfI5Ej"
   },
   "source": [
    "#Results:-\n",
    "**for LinearRegression**\\\n",
    "Rmse =  4.22823975454173\\\n",
    "Rmse cross val scores = \n",
    " [4.21674442 4.26026816 5.1071608  3.82881892 5.34093789 4.3785611\n",
    " 7.47384779 5.48226252 4.14885722 6.0669122 ]\n",
    "\n",
    "**for DecisionTreeRegressor**\\\n",
    "Rmse =  4.579825752315106\\\n",
    "Rmse cross val scores = \n",
    " [4.00051826 3.99264568 5.5571751  4.12458424 4.09615674 2.79986607\n",
    " 5.19155564 3.746465   3.25326759 4.28893926]\n",
    "\n",
    " **for SGDRegressor**\\\n",
    " Rmse =  4.252223739738116\\\n",
    "Rmse cross val scores = \n",
    " [4.24276292 4.20132664 5.16754377 3.81191694 5.30153767 4.41060904\n",
    " 7.56190827 5.47565146 4.16871895 6.12701213]\n",
    "\n",
    " **for KNeighborsRegressor**\\\n",
    " Rmse =  4.312169282573288\\\n",
    "Rmse cross val scores = \n",
    " [3.61967119 4.70763158 5.16523463 4.45849805 4.6198777  3.46735923\n",
    " 8.47209655 5.52478235 3.22961608 4.1353174 ]\n",
    "\n",
    "**for RandomForestRegressor**\\\n",
    "Rmse =  3.3789639901985953\\\n",
    "Rmse cross val scores = \n",
    " [2.87667815 2.90645052 4.44626112 2.62887072 3.43234611 2.54121222\n",
    " 4.512502   3.27714691 3.44440933 3.21893902]\n",
    "\n",
    " **Here we can clearly see that our *RandomForestRegressor* model Wins here(as it is giving lowest rmse)**  \n",
    "\n",
    " üôÇüôÇ\n",
    "\n",
    " (Any edits/suggestions are most welcome) üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnhUHFirKm0t"
   },
   "source": [
    "***¬©Abhijit Kumar (NIT Duragapur CSE)***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Boston House Price predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
